# Proyecto: Pipeline de Datos y API REST para Pel铆culas y Series

##  Descripci贸n
Este proyecto implementa un **pipeline completo de datos**, desde el an谩lisis exploratorio (EDA), pasando por un proceso de **ETL**, hasta la **exposici贸n de los datos mediante una API REST** desarrollada con FastAPI.

Se procesan datasets de distintas plataformas de streaming (CSV y JSON), se aplican procesos de limpieza, normalizaci贸n y consolidaci贸n, y el dataset resultante es expuesto mediante endpoints de solo lectura (GET).

---

##  Flujo del Proyecto

1. **EDA (Exploratory Data Analysis)**  
   An谩lisis exploratorio de los datasets originales para detectar inconsistencias, valores faltantes y definir reglas de normalizaci贸n.

2. **ETL (Extract, Transform, Load)**  
   - **Extract:** lectura de datos desde archivos CSV/JSON  
   - **Transform:** limpieza, normalizaci贸n de categor铆as y unificaci贸n de formatos  
   - **Load:** generaci贸n de un dataset consolidado en CSV

3. **API REST**  
   La API carga el dataset procesado una 煤nica vez en memoria y expone endpoints para consultas anal铆ticas.

---

##  Tecnolog铆as Utilizadas
- **Lenguaje:** Python  
- **Procesamiento de datos:** pandas, numpy  
- **API:** FastAPI  
- **Contenerizaci贸n:** Docker  
- **Almacenamiento:** Archivos CSV y JSON  

---

##  Estructura del Proyecto

```text
app/
 main.py                # API FastAPI (endpoints GET)

Datos/
 archivos/              # Datasets originales (raw)
 procesados/
     df_procesado.csv   # Dataset final generado por el ETL

ETL/
 extraer.py             # Extracci贸n de datos
 transformar.py         # Transformaciones y normalizaci贸n
 cargar.py              # Carga del dataset final
 etl.py                 # Orquestador del proceso ETL
 EDA.ipynb              # An谩lisis exploratorio de datos

Dockerfile
requirements.txt
README.md
````

---

## 锔 Ejecuci贸n del ETL

Desde la ra铆z del proyecto:

```bash
python ETL/etl.py
```

Este proceso genera el archivo:

```text
Datos/procesados/df_procesado.csv
```

---

##  Ejecuci贸n de la API por Consola

```bash
uvicorn app.main:app --reload
```
*Para detener el programa* `ctrl + C`.

##  Ejecuci贸n de la API con Docker

###  Requisitos

- Tener Docker instalado
- Estar ubicado en la ra铆z del proyecto (donde est谩 el `Dockerfile`)

---

**Construir la imagen**

```bash
docker build -t fastapi-app .
```
**Ejecutar el contenedor**
```bash
docker run -p 8000:80 --name api-peliculas fastapi-app
```

*Para detener el programa* `ctrl + C`.

---
### Ejecutar al terminar de usar la api
**Detener el contenedor**

```bash
docker stop api-peliculas
```
**Eliminar el contenedor**

```bash
docker rm -f api-peliculas
```
---
##  Acceder a la API

**Abrir en el navegador**

```bash
http://localhost:8000
```
**Documentaci贸n interactiva**

```bash
http://localhost:8000/docs
```
---

##  Endpoints Disponibles

| Endpoint              | Descripci贸n                                            |
| --------------------- | ------------------------------------------------------ |
| `/get_max_duration`   | T铆tulo con mayor duraci贸n seg煤n a帽o, plataforma y tipo |
| `/get_count_platform` | Cantidad de t铆tulos por plataforma                     |
| `/get_listedin`       | Plataforma con m谩s t铆tulos de un g茅nero                |
| `/get_actor`          | Actor m谩s frecuente por plataforma y a帽o               |

---

##  Alcance y Decisiones de Dise帽o

* API de solo lectura (GET)
* Dataset cargado una sola vez en memoria
* Separaci贸n clara entre EDA, ETL y API
* Dise帽o simple y reproducible, preparado para escalar a base de datos si el volumen lo requiere

---

##  Comentario Final

El proyecto prioriza **claridad, buenas pr谩cticas y separaci贸n de responsabilidades**, evitando complejidad innecesaria dada la escala del dataset y el tipo de consultas.
